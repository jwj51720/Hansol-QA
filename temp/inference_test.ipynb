{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from peft import (\n",
    "    AutoPeftModelForCausalLM,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel\n",
    ")\n",
    "import re\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- peft 학습한 것 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"../result/DataVortexS-10.7B-dpo-v1.11_2024-03-05_02-48-43/best\"\n",
    "load_tokenizer_name = \"../result/DataVortexS-10.7B-dpo-v1.11_2024-03-05_02-48-43\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=False,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=\"float16\",\n",
    "            )\n",
    "\n",
    "models = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config)\n",
    "models = PeftModel.from_pretrained(models, peft_model_id)\n",
    "models = models.merge_and_unload() # 7481 -> 8047\n",
    "\n",
    "tokenizers = AutoTokenizer.from_pretrained(load_tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing `quantization_config` argument at the same time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m      2\u001b[0m bnb_config \u001b[39m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      3\u001b[0m             load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m             bnb_4bit_use_double_quant\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m             bnb_4bit_quant_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnf4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m             bnb_4bit_compute_dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat16\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         )\n\u001b[1;32m      8\u001b[0m lora_config \u001b[39m=\u001b[39m LoraConfig(\n\u001b[1;32m      9\u001b[0m                 lora_alpha\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m     10\u001b[0m                 lora_dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCAUSAL_LM\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m             )\n\u001b[0;32m---> 15\u001b[0m models \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, revision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mv1.1\u001b[39;49m\u001b[39m\"\u001b[39;49m, quantization_config\u001b[39m=\u001b[39;49mbnb_config, load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m models\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m models\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2952\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[39mif\u001b[39;00m load_in_4bit \u001b[39mor\u001b[39;00m load_in_8bit:\n\u001b[1;32m   2951\u001b[0m     \u001b[39mif\u001b[39;00m quantization_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2952\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2953\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`quantization_config` argument at the same time.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2955\u001b[0m         )\n\u001b[1;32m   2957\u001b[0m     \u001b[39m# preparing BitsAndBytesConfig from kwargs\u001b[39;00m\n\u001b[1;32m   2958\u001b[0m     config_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(BitsAndBytesConfig)\u001b[39m.\u001b[39mparameters}\n",
      "\u001b[0;31mValueError\u001b[0m: You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing `quantization_config` argument at the same time."
     ]
    }
   ],
   "source": [
    "model_id = \"LDCC/LDCC-SOLAR-10.7B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\",\n",
    "        )\n",
    "lora_config = LoraConfig(\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                r=8,\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "            )\n",
    "models = AutoModelForCausalLM.from_pretrained(model_id, revision=\"v1.1\", quantization_config=bnb_config)\n",
    "models.config.use_cache = False\n",
    "models.config.pretraining_tp = 1\n",
    "models.enable_input_require_grads()\n",
    "model = get_peft_model(models, lora_config)\n",
    "\n",
    "# tokenizers = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test_split.csv\")\n",
    "\n",
    "print(tokenizers.pad_token)\n",
    "print(tokenizers.eos_token)\n",
    "\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요?'\n",
    "\n",
    "with open(\"../template/datavortex.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "answer_start_index = content.find(\"<answer>\")\n",
    "content = content[:answer_start_index]\n",
    "content = content.replace(\"<question>\", input_text)\n",
    "\n",
    "input_ids = tokenizers.encode(\n",
    "            content, padding=False, max_length=256, return_tensors=\"pt\", add_special_tokens=False\n",
    "        )\n",
    "print(content, \"\\n--------------\\n\")\n",
    "print(content.replace(\"\\n\", \"\\\\n\"), \"\\n--------------\\n\")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequences = models.generate(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        max_length=512,\n",
    "        temperature=0.2,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "print(tokenizers.decode(output_sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.encode([\"방청 페인트는 다양한 종류로 분류됩니다 . 그 중에서 가장 흔 히 사용되는 것은 광 명단 페인 트, 방청산화철 페인 트, 알루미늄 페인 트, 역청질 페인 트, 워시 프라이머 , 크롬 산아연 페인 트 등이 있습니다 . 이러한 다양한 종류 의 방청 페인트가 각자의 특성과 용도에 맞 게 사용됩니다 .\"])\n",
    "t = model.encode([\"방청 페인트는 다양한 종류로 분류됩니다. 그 중에서 가장 흔히 사용되는 것은 광명단 페인트, 방청산화철 페인트, 알루미늄 페인트, 역청질 페인트, 워시 프라이머, 크롬 산아연 페인트 등이 있습니다. 이러한 다양한 종류의 방청 페인트가 각자의 특성과 용도에 맞게 사용됩니다.\"])\n",
    "cosine_similarity(k, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start token이 잘못되어 보정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(CFG, preds):\n",
    "    model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
    "    nl = pd.read_csv(f\"../{CFG['DATA_PATH']}/{CFG['TEST_DATA']}\")\n",
    "    submit = pd.read_csv(f\"../{CFG['DATA_PATH']}/{CFG['SUBMISSION_DATA']}\")\n",
    "    submission_name = \"DataVortexS-10.7B-dpo-v1.11_2024-03-04_02-11-13\"\n",
    "    nl[\"답변\"] = preds\n",
    "    nl.to_csv(f'../{CFG[\"SAVE_PATH\"]}/{submission_name}/NL_{submission_name}.csv', index=False)\n",
    "    if len(nl) != len(submit):\n",
    "        nl = (\n",
    "            nl.groupby(\"id\")[\"답변\"]\n",
    "            .apply(lambda x: \" \".join(x.astype(str)))\n",
    "            .reset_index()\n",
    "        )\n",
    "        preds = nl[\"답변\"]\n",
    "        nl.to_csv(f'../{CFG[\"SAVE_PATH\"]}/{submission_name}/NL_merge_{submission_name}.csv', index=False)\n",
    "    pred_embeddings = model.encode(preds)\n",
    "    print(\"Shape of Prediction Embeddings: \", pred_embeddings.shape)\n",
    "    submit.iloc[:, 1:] = pred_embeddings\n",
    "    submit.to_csv(f'../{CFG[\"SAVE_PATH\"]}/{submission_name}/{submission_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../config/datavortex_0304.json', 'r', encoding='utf-8') as file:\n",
    "    CFG = json.load(file)\n",
    "    \n",
    "infer_csv = pd.read_csv(\"../result/NL_DataVortexS-10.7B-dpo-v1.11_2024-03-04_02-11-13.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "start_token = \"### Assistant: \"\n",
    "for full_text in infer_csv['답변']:\n",
    "    answer_start = full_text.find(start_token) + len(start_token)\n",
    "    answer_only = full_text[answer_start:].strip()\n",
    "    answer_only = answer_only.replace(\"\\n\", \"\")\n",
    "    answer_only = re.sub(r'\\s+', ' ', answer_only)\n",
    "    answer_only = answer_only.replace(\"<|im_end|>\", \"\")\n",
    "    answer_only = answer_only.replace(\"</s>\", \"\")\n",
    "    preds.append(answer_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(CFG, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
